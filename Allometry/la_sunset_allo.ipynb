{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PatchCollection\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.19.2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.version.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading tree data, allometric equations, etc... ##\n",
    "Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# path to file\n",
    "home = os.path.expanduser('~')\n",
    "path = f'{home}/UrbanForest/all_clean_LAcounty_sunset.hdf'\n",
    "\n",
    "# read the hdf\n",
    "la = pd.read_hdf(path, key='data')\n",
    "\n",
    "# select desired columns\n",
    "cols=['ID', 'LATITUDE', 'LONGITUDE', 'DBH_LO', 'DBH_HI', 'CREATED',\n",
    "      'UPDATED', 'SOURCE', 'Name_matched', 'Zone']\n",
    "la = la[cols]\n",
    "\n",
    "# drop NAs\n",
    "la.dropna(how='any', axis=0, subset=['DBH_LO', 'DBH_HI'], inplace=True)\n",
    "\n",
    "# capitalize genus names\n",
    "la['Name_matched'] = la.Name_matched.str.capitalize()\n",
    "\n",
    "# convert DBH to cm\n",
    "la['dbh_low']  = 2.54 * la.DBH_LO\n",
    "la['dbh_high'] = 2.54 * la.DBH_HI\n",
    "la.drop(['DBH_LO', 'DBH_HI'], axis=1, inplace=True)\n",
    "\n",
    "# Change date fields to dateTime type\n",
    "la['created'] = pd.to_datetime(la.CREATED)\n",
    "la['updated'] = pd.to_datetime(la.UPDATED)\n",
    "la.drop(['CREATED', 'UPDATED'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will first use allometric equations from :\n",
    "\n",
    " McPherson, E. Gregory; van Doorn, Natalie S.; Peper, Paula J. 2016. Urban tree database.\n",
    " Fort Collins, CO: Forest Service Research Data Archive. Updated 21 January 2020.\n",
    " https://doi.org/10.2737/RDS-2016-0005\n",
    "\n",
    " 'Apps min' and 'Apps max' give the input range (cm) that the authors feel \n",
    "  that the equations are reliable\n",
    " 'InlEmp' and 'SoCalC' are Climate zones where the eqs are different.\n",
    "  SoCalC reference city is Santa Monica, InlEmp is Claremont,\n",
    "  see Table 1, p16 for further Climate zone details.  \n",
    "  \n",
    "  After reading the equations and coefficients, we will get rid of trees that only occur a few times, and trees that we o not have equations for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 664155 entries, 0 to 1089845\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   ID            664155 non-null  int64         \n",
      " 1   LATITUDE      664155 non-null  float64       \n",
      " 2   LONGITUDE     664155 non-null  float64       \n",
      " 3   SOURCE        664155 non-null  object        \n",
      " 4   Name_matched  664155 non-null  object        \n",
      " 5   Zone          663384 non-null  float64       \n",
      " 6   dbh_low       664155 non-null  float64       \n",
      " 7   dbh_high      664155 non-null  float64       \n",
      " 8   created       28472 non-null   datetime64[ns]\n",
      " 9   updated       28472 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(5), int64(1), object(2)\n",
      "memory usage: 55.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# The equations\n",
    "def mcpherson_eqs():\n",
    "    '''returns dict of equations from table 3 (p24) of McPherson 2020\n",
    "    functions use np so as to be vectorized'''\n",
    "\n",
    "    eq_dict = {'lin'        : (lambda a, b, c, d, e, x, mse: a + b * (x)), \n",
    "                'quad'      : (lambda a, b, c, d, e, x, mse: a + b * x + c * x**2),\n",
    "                'cub'      : (lambda a, b, c, d, e, x, mse: a + b * x + c * x**2 + d * x**3),\n",
    "                'quart'     : (lambda a, b, c, d, e, x, mse:a + b * x + c *x**2 + d * x**3 + e * x**4), \n",
    "                'loglogw1' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1) + (mse/2)))),\n",
    "                'loglogw2' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1)) + (np.sqrt(x) + (mse/2)))),\n",
    "                'loglogw3' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1)) + (x) + (mse/2))),\n",
    "                'loglogw4' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1)) + (x**2) + (mse/2))),\n",
    "                'expow1'    : (lambda a, b, c, d, e, x, mse: np.exp(a+ b * (x) + (mse/2))),\n",
    "                'expow2'    : (lambda a, b, c, d, e, x, mse: np.exp(a + b * (x) + np.sqrt(x) + (mse/2))),\n",
    "                'expow3'    : (lambda a, b, c, d, e, x, mse: np.exp(a + b * (x) + (x) + (mse/2))),\n",
    "                'expow4'    : (lambda a, b, c, d, e, x, mse: np.exp(a + b * (x) + (x**2) + (mse/2)))}\n",
    "\n",
    "    return(eq_dict)\n",
    "\n",
    "eq_dict = mcpherson_eqs()\n",
    "\n",
    "# The cooeficients\n",
    "coef_df = pd.read_csv('TS6_Growth_coefficients.csvx',\n",
    "usecols=['Region', 'Scientific Name', 'Independent variable', 'Predicts component ', 'EqName', 'Units of predicted components',\n",
    "'EqName', 'a', 'b', 'c', 'd', 'e', 'Apps min', 'Apps max'])\n",
    "\n",
    "# Find all the trees with over 100 occurances in the dataset\n",
    "trees = la.Name_matched.value_counts()\n",
    "trees = list(trees.where(trees > 100).dropna().index)\n",
    "\n",
    "# drop trees we do not have equations for\n",
    "trees = [s for s in trees if s in coef_df['Scientific Name'].unique()]\n",
    "la = la.loc[la.Name_matched.isin(trees)]\n",
    "\n",
    "la.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Lidar data ## \n",
    "We will be using the ```USGS_LPC_CA_LosAngeles_2016_LAS_2018``` dataset.  The USGS lidar data is hosted in an Amazon S3 bucket, so we will need the AWS client to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl \"https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\" -o \"awscli-bundle.zip\"\n",
    "#!unzip awscli-bundle.zip \n",
    "#!./awscli-bundle/install -b ~/bin/aws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a tmp directory too, if we don't already have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jovyan/tmp’: File exists\n"
     ]
    }
   ],
   "source": [
    "# make a tmp directory\n",
    "\n",
    "!mkdir ~/tmp\n",
    "\n",
    "# make a variable for its path\n",
    "tmp = f'{home}/tmp'\n",
    "\n",
    "# make a variable with the path to aws cli\n",
    "aws = '~/bin/aws'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the ept metadata ###\n",
    "The top level ept json for the ```USGS_LPC_CA_LosAngeles_2016_LAS_2018``` dataset contains important metadata. We will download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='~/bin/aws s3 cp s3://usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json /home/jovyan/tmp --no-sign-request', returncode=0, stdout=b'Completed 2.4 KiB/2.4 KiB (7.8 KiB/s) with 1 file(s) remaining\\rdownload: s3://usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json to ../tmp/ept.json\\n', stderr=b'')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "cmd = f'{aws} s3 cp s3://usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json {tmp} --no-sign-request'\n",
    "subprocess.run(cmd, shell=True, capture_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load ```ept.json``` and extract usefull information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataType is: laszip\n",
      "hierarchyType is: json\n",
      "srs is:\n",
      "{'authority': 'EPSG', 'horizontal': '3857', 'wkt': 'PROJCS[\"WGS 84 / Pseudo-Mercator\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Mercator_1SP\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"scale_factor\",1],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"X\",EAST],AXIS[\"Y\",NORTH],EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs\"],AUTHORITY[\"EPSG\",\"3857\"]]'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(f'{tmp}/ept.json') as f:\n",
    "    meta = json.load(f)\n",
    "    \n",
    "bounds = meta['bounds']\n",
    "bounds_conf = meta['boundsConforming']\n",
    "srs = meta['srs']\n",
    "span   = meta['span']\n",
    "schema  = meta['schema']\n",
    "dataType = meta['dataType']\n",
    "hierarchyType = meta['hierarchyType']\n",
    "\n",
    "def bag_scale_offset(name, schema):\n",
    "    '''Retruns scale and offset for the spatial dimension given by name'''\n",
    "    for thing in schema:\n",
    "        if thing['name'] == name:\n",
    "            return(thing['scale'], thing['offset'])\n",
    "        \n",
    "x_scale, x_offset = bag_scale_offset('X', schema)\n",
    "y_scale, y_offset = bag_scale_offset('Y', schema)\n",
    "z_scale, z_offset = bag_scale_offset('Z', schema)\n",
    "\n",
    "#print some info\n",
    "print(f'dataType is: {dataType}\\nhierarchyType is: {hierarchyType}\\nsrs is:\\n{srs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output tells us the data is in EPSG:3857.  There is only a horizontal code present.  Lets reduce the srs to a more useful form for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EPSG:3857'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srs = meta['srs']['authority'] + ':' + meta['srs']['horizontal']\n",
    "srs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the srs is in Pseudo-Mercator (EPSG:3857), and our data is lat, lon (EPSG:4326) We will have to reproject. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying data ###\n",
    "Now it should be possible to define a bounding box around a tree to query the ept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import osr\n",
    "\n",
    "# For now we will ad 0.00007 degrees in each direction, this is jus a guess based on 5th decimal place ~ 1.1m\n",
    "# also not setting z max and min for the moment\n",
    "def make_scaled_bbox(lat, lon, bounds=None):\n",
    "    '''Returns a bbox in ept coords.\n",
    "    If present bounds is of form [xmin, ymin, zmin, xmax, ymax, zmax]'''\n",
    "    # make the bbox in EPSG:4326\n",
    "    [xmin, xmax], [ymin, ymax] = make_bbox(lat, lon)\n",
    "    \n",
    "    # define transform from EPSG:4326 to EPSG:3857\n",
    "    old_crs = osr.SpatialReference() \n",
    "    old_crs.ImportFromEPSG(4326) \n",
    "    new_crs = osr.SpatialReference() \n",
    "    new_crs.ImportFromEPSG(3857)\n",
    "    transform = osr.CoordinateTransformation(old_crs,new_crs)\n",
    "    \n",
    "    # transform bbox points\n",
    "    xmin, ymin, zmin = transform.TransformPoint(ymin, xmin)\n",
    "    xmax, ymax, zmax = transform.TransformPoint(ymax, xmax)\n",
    "\n",
    "    # TODO:make sure no bbox is out of the ept bbox, for edge cases\n",
    "    # if bounds:\n",
    "        #blah for blah in blah\n",
    "    \n",
    "    return([xmin, xmax], [ymin, ymax])\n",
    "\n",
    "def make_bbox(lat, lon):\n",
    "    buf = 0.00007\n",
    "    xmin = lon - buf\n",
    "    ymin = lat - buf\n",
    "    xmax = lon + buf\n",
    "    ymax = lat + buf\n",
    "    return([xmin, xmax], [ymin, ymax])\n",
    "\n",
    "def bbox_geojson(lat, lon, filename):\n",
    "    '''makes wgs84 bbox as geojson for comparison in gis'''\n",
    "    [xmin, xmax], [ymin, ymax] = make_bbox(lat, lon)\n",
    "    gjson = {'coordinates' : [[[xmin, ymin], [xmin, ymax], [xmax, ymax], [xmax, ymin]]],\n",
    "            'type' : 'Polygon'}\n",
    "    with open(filename, 'w') as of:\n",
    "        json.dump(gjson, of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "had to run \n",
    "```conda update numpy```\n",
    "in terminal first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdal in /opt/conda/lib/python3.8/site-packages (2.3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to get the point cloud within the bbox using PDALs ept reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-66f89e03467c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpdal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pdal/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2.3.7'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdimension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pdal/pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibpdalpython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mlibpdalpython.pyx\u001b[0m in \u001b[0;36minit pdal.libpdalpython\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "import pdal\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test things out we will select a single tree.  ```id``` is the uniqued ID for the tree in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lat, lon of point from dataset\n",
    "uid = 2181656\n",
    "lat = la.loc[la.ID==uid]['LATITUDE'].values[0]\n",
    "lon = la.loc[la.ID==uid]['LONGITUDE'].values[0]\n",
    "species = la.loc[la.ID==uid]['Name_matched'].values[0]\n",
    "\n",
    "# get bbox coords in EPSG:4326\n",
    "[xmin, xmax], [ymin, ymax] = make_bbox(lat, lon)\n",
    "\n",
    "# make geojson of bbox in EPSG:4326\n",
    "bbox_geojson(lat, lon, f'bbox_{id}.json')\n",
    "\n",
    "# make transformed bbox\n",
    "scaled_bbox = make_scaled_bbox(lat, lon, bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species\n",
    "# UTM11 is EPSG:26911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and validate pipeline\n",
    "t = Template('''\n",
    "{\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "        \"bounds\": \"${scaled_bbox}\",\n",
    "        \"filename\": \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json\",\n",
    "        \"type\": \"readers.ept\",\n",
    "        \"tag\": \"readdata\",\n",
    "        \"spatialreference\":\"${srs}\"\n",
    "        },\n",
    "        {\n",
    "        \"type\":\"filters.outlier\",\n",
    "        \"method\":\"radius\",\n",
    "        \"radius\":1.0,\n",
    "        \"min_k\":4\n",
    "        },\n",
    "        {\n",
    "        \"type\": \"filters.reprojection\",\n",
    "        \"in_srs\":\"${srs}\",\n",
    "        \"out_srs\": \"EPSG:26911\"\n",
    "        },\n",
    "        {\n",
    "        \"filename\": \"dsm${uid}.tif\",\n",
    "        \"gdalopts\": \"tiled=yes,     compress=deflate\",\n",
    "        \"nodata\": -9999,\n",
    "        \"output_type\": \"idw\",\n",
    "        \"resolution\": 1,\n",
    "        \"type\": \"writers.gdal\",\n",
    "        \"window_size\": 6\n",
    "        },\n",
    "        {\n",
    "        \"type\":\"filters.smrf\",\n",
    "        \"scalar\":1.2,\n",
    "        \"slope\":0.2,\n",
    "        \"threshold\":0.45,\n",
    "        \"window\":16.0\n",
    "        },\n",
    "        {\n",
    "        \"type\":\"filters.hag_delaunay\"\n",
    "        },\n",
    "        {\n",
    "        \"filename\": \"chm${uid}.tif\",\n",
    "        \"gdalopts\": \"tiled=yes,     compress=deflate\",\n",
    "        \"nodata\": -9999,\n",
    "        \"output_type\": \"idw\",\n",
    "        \"resolution\": 1,\n",
    "        \"type\": \"writers.gdal\",\n",
    "        \"window_size\": 6,\n",
    "        \"dimension\": \"HeightAboveGround\"\n",
    "        }\n",
    "    ]\n",
    "}''')\n",
    "\n",
    "pipe = t.substitute(scaled_bbox=scaled_bbox, srs=srs, uid=uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pdal.Pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pipeline.arrays[0]\n",
    "metadata = pipeline.metadata\n",
    "log = pipeline.log\n",
    "S[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the pipline made a DSM for the query box.  We also now have the returns in an np structured array, ```S```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at a histogaram of _hag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg = S[S['Classification']!=2]\n",
    "h = veg['HeightAboveGround'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(h, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the historgram above there are a bunch of < 2m returns.  Here is a (huge) picture of the tree (uid = 2181656) from Google street view.  \n",
    "<img src=\"streetView_2181656.png\">\n",
    "\n",
    "It seems like those < 2m returns must be from parked cars, or some kind of noise, or some other not-useful thing. Parked cars a re a likely culprit, and they will be trivky to deal with in a systematic way.  For now I am going to just ditch returns below 2m. On small trees this may end up causing issues, we'll see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg = S[S['HeightAboveGround']>2]\n",
    "h = veg['HeightAboveGround'].flatten()\n",
    "plt.hist(h, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, that looks like half a tree lying on its side!  Next, I write more functions solidifying the work we have done above to make it easiert o do to many trees.  Lets also add some new dimensions to the points using some filters in the PDAL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipe(bbox, uid, dsm_path, chm_path, srs='EPSG:3857'):\n",
    "    '''Creates, validates and then returns the pdal pipeline\n",
    "    \n",
    "    Arguments:\n",
    "    bbox     -- Bounding box in srs coordintes (default srs is EPSG:3857),\n",
    "                in the form: ([xmin, xmax], [ymin, ymax]).\n",
    "    uid      -- int - ID of the desired tree in the dataframe.\n",
    "    dsm_path -- String - Path where the DSM shall be saved. Must include .tif exstension.\n",
    "    chm_path -- String - Path where the CHM shall be saved. Must include .tif exstension.\n",
    "    srs      -- String - EPSG identifier for srs  being used. Defaults to EPSG:3857\n",
    "                because that is what ept files tend to use.\n",
    "    '''\n",
    "    \n",
    "    t = Template('''\n",
    "    {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "            \"bounds\": \"${scaled_bbox}\",\n",
    "            \"filename\": \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json\",\n",
    "            \"type\": \"readers.ept\",\n",
    "            \"tag\": \"readdata\",\n",
    "            \"spatialreference\":\"${srs}\"\n",
    "            },\n",
    "            {\n",
    "            \"type\":\"filters.outlier\",\n",
    "            \"method\":\"radius\",\n",
    "            \"radius\":1.0,\n",
    "            \"min_k\":4\n",
    "            },\n",
    "            {\n",
    "            \"type\": \"filters.reprojection\",\n",
    "            \"in_srs\":\"${srs}\",\n",
    "            \"out_srs\": \"EPSG:26911\"\n",
    "            },\n",
    "            {\n",
    "            \"filename\": \"${dsm_path}\",\n",
    "            \"gdalopts\": \"tiled=yes,     compress=deflate\",\n",
    "            \"nodata\": -9999,\n",
    "            \"output_type\": \"idw\",\n",
    "            \"resolution\": 1,\n",
    "            \"type\": \"writers.gdal\",\n",
    "            \"window_size\": 6\n",
    "            },\n",
    "            {\n",
    "            \"type\":\"filters.smrf\",\n",
    "            \"scalar\":1.2,\n",
    "            \"slope\":0.2,\n",
    "            \"threshold\":0.45,\n",
    "            \"window\":16.0\n",
    "            },\n",
    "            {\n",
    "            \"type\":\"filters.hag_delaunay\"\n",
    "            },\n",
    "            {\n",
    "            \"filename\": \"${chm_path}\",\n",
    "            \"gdalopts\": \"tiled=yes,     compress=deflate\",\n",
    "            \"nodata\": -9999,\n",
    "            \"output_type\": \"idw\",\n",
    "            \"resolution\": 1,\n",
    "            \"type\": \"writers.gdal\",\n",
    "            \"window_size\": 6,\n",
    "            \"dimension\": \"HeightAboveGround\"\n",
    "            },\n",
    "            {\n",
    "            \"type\":\"filters.optimalneighborhood\",\n",
    "            \"min_k\":8,\n",
    "            \"max_k\": 50\n",
    "            },\n",
    "            {\n",
    "            \"type\":\"filters.covariancefeatures\",\n",
    "            \"knn\":10,\n",
    "            \"threads\": 2,\n",
    "            \"feature_set\": \"all\"\n",
    "            }\n",
    "        ]\n",
    "    }''')\n",
    "\n",
    "    pipe = t.substitute(scaled_bbox=bbox, srs=srs, uid=uid, dsm_path=dsm_path, chm_path=chm_path)\n",
    "    pipeline = pdal.Pipeline(pipe)\n",
    "    if pipeline.validate():\n",
    "        return(pipeline)\n",
    "    else:\n",
    "        raise Exception('Bad pipeline (sorry to be so ambigous)!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_path = f'{tmp}/dsm{uid}.tif'\n",
    "chm_path = f'{tmp}/chm{uid}.tif'\n",
    "\n",
    "\n",
    "pipeline = make_pipe(scaled_bbox, uid, dsm_path, chm_path)\n",
    "count = pipeline.execute()\n",
    "S = pipeline.arrays[0]\n",
    "metadata = pipeline.metadata\n",
    "log = pipeline.log\n",
    "veg = S[(S['Classification']!=2) & (S['HeightAboveGround']>2)]\n",
    "\n",
    "veg = veg[['Intensity',\n",
    "           'NumberOfReturns',\n",
    "           'ReturnNumber',\n",
    "           'X',\n",
    "           'Y',\n",
    "           'HeightAboveGround',\n",
    "           'OptimalKNN',\n",
    "           'OptimalRadius', \n",
    "           'Linearity', \n",
    "           'Planarity', \n",
    "           'Scattering', \n",
    "           'Verticality', \n",
    "           'Omnivariance', \n",
    "           'Anisotropy', \n",
    "           'Eigenentropy', \n",
    "           'EigenvalueSum', \n",
    "           'SurfaceVariation', \n",
    "           'DemantkeVerticality',\n",
    "           'Density' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it a dataframe for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(veg)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the centroid of highest n points. Then change x and y to be relative to the centroid. Compute radius from that centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "tops = df[['X', 'Y', 'HeightAboveGround']].sort_values(ascending=False, by='HeightAboveGround')\n",
    "x = tops.head(n).X.mean()\n",
    "y = tops.head(n).Y.mean()\n",
    "\n",
    "df['X'] = df.X - x\n",
    "df['Y'] = df.Y - y\n",
    "df['r'] = np.sqrt(df.X**2 + df.Y**2)\n",
    "df.sort_values(ascending=True, by='r', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if there is an obvious trend between r and height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='r', y='HeightAboveGround');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really.  Make a tiny dataset in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = [2181656, 2181657, 2181697, 2181698, 2181699, 2184893]\n",
    "tiny_data = la.loc[la.ID.isin(uids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a loop of it and parrallelize it etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "\n",
    "@delayed\n",
    "def row_bounds_ept_query(data, idx):\n",
    "    # get lat lon of first entry\n",
    "    row = data.iloc[idx]\n",
    "    uid = row['ID']\n",
    "    lat = row['LATITUDE']\n",
    "    lon = row['LONGITUDE']\n",
    "\n",
    "    # make bbox in EPSG:3857\n",
    "    scaled_bbox = make_scaled_bbox(lat, lon, bounds=bounds_conf)\n",
    "    \n",
    "    # sanity check will raise erros if scalled bbox is not in the ept bounds\n",
    "    assert (scaled_bbox[0][0] > bounds[0]) & (scaled_bbox[0][1] < bounds[3])\n",
    "    assert (scaled_bbox[1][0] > bounds[1]) & (scaled_bbox[1][1] < bounds[4])\n",
    "    \n",
    "    # make and validate pipeline\n",
    "    dsm_path = f'{tmp}/dsm{uid}.tif'\n",
    "    chm_path = f'{tmp}/chm{uid}.tif'\n",
    "\n",
    "    # make and validate pipeline\n",
    "    pipeline = make_pipe(scaled_bbox, uid, dsm_path, chm_path)\n",
    "    \n",
    "    # execute the piepline\n",
    "    count = pipeline.execute()\n",
    "    S = pipeline.arrays[0]\n",
    "    metadata = pipeline.metadata\n",
    "    log = pipeline.log\n",
    "    \n",
    "    #Trim the pointcloud to what we want to use\n",
    "    veg = S[(S['Classification']!=2) & (S['HeightAboveGround']>2)]\n",
    "\n",
    "    # do stuff\n",
    "    trim = count - veg.shape[0]\n",
    "    count = veg.shape[0]\n",
    "    if count > 0:\n",
    "        # make it a dataframe\n",
    "        df = pd.DataFrame(veg)\n",
    "        # find the centroid of highest n points. \n",
    "        n = 5\n",
    "        tops = df[['X', 'Y', 'HeightAboveGround']].sort_values(ascending=False, by='HeightAboveGround')\n",
    "        x = tops.head(n).X.mean()\n",
    "        y = tops.head(n).Y.mean()\n",
    "        # change x and y to be relative to the centroid. Compute radius from that centroid.\n",
    "        df['X'] = df.X - x\n",
    "        df['Y'] = df.Y - y\n",
    "        df['r'] = np.sqrt(df.X**2 + df.Y**2)\n",
    "        # sort by r\n",
    "        df.sort_values(ascending=True, by='r', inplace=True)\n",
    "        \n",
    "        return(df)\n",
    "        \n",
    "results = []\n",
    "for i in range(len(tiny_data)):\n",
    "    results.append(row_bounds_ept_query(tiny_data, i))\n",
    "    \n",
    "with ProgressBar():\n",
    "    S = compute(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[1].plot.scatter(x='r', y='HeightAboveGround');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[2].plot.scatter(x='r', y='HeightAboveGround');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[3].plot.scatter(x='r', y='HeightAboveGround');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[4].plot.scatter(x='r', y='HeightAboveGround');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildVRT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrcDSOrSrcDSTab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Build a VRT from a list of datasets.\n",
       "Arguments are :\n",
       "  destName --- Output dataset name\n",
       "  srcDSOrSrcDSTab --- an array of Dataset objects or filenames, or a Dataset object or a filename\n",
       "Keyword arguments are :\n",
       "  options --- return of gdal.BuildVRTOptions(), string or array of strings\n",
       "  other keywords arguments of gdal.BuildVRTOptions()\n",
       "If options is provided as a gdal.BuildVRTOptions() object, other keywords are ignored. \n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.8/site-packages/osgeo/gdal.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?gdal.BuildVRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chm40.tif',\n",
       " 'chm50.tif',\n",
       " 'chm79.tif',\n",
       " 'chm90.tif',\n",
       " 'chm74.tif',\n",
       " 'chm84.tif',\n",
       " 'chm88.tif',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'acbsb.fork'.endswith('.fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/tmp'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Read a dataset's raw pixels as an N-d array\n",
       "\n",
       "This data is read from the dataset's band cache, which means\n",
       "that repeated reads of the same windows may avoid I/O.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "indexes : list of ints or a single int, optional\n",
       "    If `indexes` is a list, the result is a 3D array, but is\n",
       "    a 2D array if it is a band index number.\n",
       "\n",
       "out : numpy ndarray, optional\n",
       "    As with Numpy ufuncs, this is an optional reference to an\n",
       "    output array into which data will be placed. If the height\n",
       "    and width of `out` differ from that of the specified\n",
       "    window (see below), the raster image will be decimated or\n",
       "    replicated using the specified resampling method (also see\n",
       "    below).\n",
       "\n",
       "    *Note*: the method's return value may be a view on this\n",
       "    array. In other words, `out` is likely to be an\n",
       "    incomplete representation of the method's results.\n",
       "\n",
       "    This parameter cannot be combined with `out_shape`.\n",
       "\n",
       "out_dtype : str or numpy dtype\n",
       "    The desired output data type. For example: 'uint8' or\n",
       "    rasterio.uint16.\n",
       "\n",
       "out_shape : tuple, optional\n",
       "    A tuple describing the shape of a new output array. See\n",
       "    `out` (above) for notes on image decimation and\n",
       "    replication.\n",
       "\n",
       "    Cannot combined with `out`.\n",
       "\n",
       "window : a pair (tuple) of pairs of ints or Window, optional\n",
       "    The optional `window` argument is a 2 item tuple. The first\n",
       "    item is a tuple containing the indexes of the rows at which\n",
       "    the window starts and stops and the second is a tuple\n",
       "    containing the indexes of the columns at which the window\n",
       "    starts and stops. For example, ((0, 2), (0, 2)) defines\n",
       "    a 2x2 window at the upper left of the raster dataset.\n",
       "\n",
       "masked : bool, optional\n",
       "    If `masked` is `True` the return value will be a masked\n",
       "    array. Otherwise (the default) the return value will be a\n",
       "    regular array. Masks will be exactly the inverse of the\n",
       "    GDAL RFC 15 conforming arrays returned by read_masks().\n",
       "\n",
       "boundless : bool, optional (default `False`)\n",
       "    If `True`, windows that extend beyond the dataset's extent\n",
       "    are permitted and partially or completely filled arrays will\n",
       "    be returned as appropriate.\n",
       "\n",
       "resampling : Resampling\n",
       "    By default, pixel values are read raw or interpolated using\n",
       "    a nearest neighbor algorithm from the band cache. Other\n",
       "    resampling algorithms may be specified. Resampled pixels\n",
       "    are not cached.\n",
       "\n",
       "fill_value : scalar\n",
       "    Fill value applied in the `boundless=True` case only. Like\n",
       "    the fill_value of numpy.ma.MaskedArray, should be value\n",
       "    valid for the dataset's data type.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Numpy ndarray or a view on a Numpy ndarray\n",
       "\n",
       "Note: as with Numpy ufuncs, an object is returned even if you\n",
       "use the optional `out` argument and the return value shall be\n",
       "preferentially used by callers.\n",
       "\u001b[0;31mType:\u001b[0m      method_descriptor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?rasterio.DatasetReader.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
